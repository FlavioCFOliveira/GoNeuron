# Generative Adversarial Network (GAN) for MNIST

This example implements a Generative Adversarial Network (GAN) to synthesize handwritten digits similar to those in the MNIST dataset. It demonstrates the adversarial training process between a **Generator** and a **Discriminator**.

## Technical Objectives

1.  **Adversarial Training Logic**: Implementing the alternating optimization loops for $G$ and $D$.
2.  **Generative Modeling**: Learning a mapping from a latent noise space (Gaussian) to the image space.
3.  **LeakyReLU and Tanh Activation**: Utilizing specific activation patterns for stable GAN convergence.

## Key Implementation Details

### 1. Generator and Discriminator Architectures
-   **Generator**: Transforms a 100D noise vector into a 784D image vector (28x28 pixels). Uses `Tanh` in the output layer to map pixels to the [-1, 1] range.
-   **Discriminator**: A binary classifier that distinguishes between real MNIST images and synthetic ones generated by $G$.

### 2. Training Loop (Adversarial)
The implementation manually manages the training steps:
1.  **Train Discriminator**: Minimize $-\log(D(x)) - \log(1 - D(G(z)))$.
2.  **Train Generator**: Minimize $-\log(D(G(z)))$.

### 3. Image Synthesis
After each training epoch, the generator is used to produce sample images, which are saved as PNG files to visualize the model's progress in learning the distribution of handwritten digits.

## Execution

```bash
go run main.go
```
